{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELEN0016 - Computer Vision \n",
    "## Project Part. 2 - Ellipse matching and performance assessment \n",
    "Antoine Rousseau, Fran√ßois Blistein, Renaud Vandeghen, Alexandre Lhoest (group 7)\n",
    "\n",
    "## 1. Description of the work\n",
    "\n",
    "The aim of the project is to develop modules to detect/match ellipses in images and to assess the performances of this modules. The image database is composed of two different types of images : \n",
    "\n",
    "  -  Annotated line images (sudoku, soccer and road)\n",
    "  -  Annotated ellipse images :  soccer field images where one will have to extract the white circles of the field marking (one central and two side half-circles) and eye infra-red images where one will have to find the pupil. \n",
    "\n",
    "The global work is subdivided into four tasks : \n",
    "\n",
    "  -  Task 2.1 : performance assessment of line segment detection. The task is to assess the performances of the line detection modules developed during the first part of the project. To do so, one will describe the metrics used and present the obtained results on these annotated images.\n",
    "  -  Task 2.2 : ellipse matching. The main objective is to detect and local elliptical structures in images using Machine Learning. Practically, two programs have to be developed : \n",
    "     - For the pupil of the eyes images, the output must be the regression parameters of the detected ellipse ;\n",
    "     - For the soccer images, the output must be the bounding box of the (1,2 or 3) detected ellipses. \n",
    "  -  Task 2.4 : performance assessment of the ellipse matching module. The goal is to assess the performances of the ellipse detection and matching module. To do so, one will describe the metrics used and present the obtained results on these annotated images. Practically, there are three quantitative assessments to measure quality of : \n",
    "     - [Classification Task] The detection of ellipses in input images ;\n",
    "     - [Regression task on bounding boxes] The obtained position of the bounding boxes of the circles of the field marking in soccer images ;\n",
    "     - [Regression task on ellipse parameters] The obtained parameters of the ellipes for the pupils in eye images. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1 : performance assessment of line segment detection\n",
    "\n",
    "During the first part of the project, the goal was to find efficient ways to extract and detect lines in five types of images : Building, PCB, Soccer, Road and Sudoku. For this purpose, some preprocessing methods were applied to the images (GaussianBlur...), then one applied edges detection method such as Canny or AdaptiveThreshold and, finally, one could increase the quality of the detections using postprocessing methods as dilation. By the end of this first part of the projet, one was able to classify edge pixels as being part of a line or not. This will be the basis of the performance assessment. \n",
    "\n",
    "Indeed, one wants to assess the performance of line segment detection developed before. To do so, a database of annotated lines images (sudoku, soccer and road) are provided. One will evaluate the performance as follows : \n",
    "  -  Based on confusion matrix definition, one will compare the line segment detection module with the annotations. For the edge pixels assumed to be part of a line segment, one will check if they belong actually to an annotated lines. If so, it will count for a true positive pixel. If not, it will count for a false positive pixels. In the same idea, a true negative pixel is a non-edge pixel that is actually annotated as being a non-edge pixel. Conversly, a false negative pixel is a non-edge pixel that is annotated to belong to a line. In order to have relative results, those number of true/false positive/negative pixels will be divided by the total number of classified edge pixels computed during the first part of the project. \n",
    "  -  Another qualitative way to assess the performance of line segment detection is to compare the detected lines at the Hough Line Transform output and the annotated lines. The goal will be to count the number of Hough lines that correspond to true lines. In order to do so, one will compare the number of pixels that belong to both a Hough Line and annotated lines. Thanks to this, it will be chosen if it actually represents the same line (with a tolerance parameter called alpha, tuned by hand for each different image types). This assessment method is less precised than the first one, but it gives a complementary evaluation. \n",
    "\n",
    "In order to get consistent results, the two methods are applied to the full image datasets for the three types of images. The edge detection methods developped during the first part of the project are assumed to be good if they give high percentage of true positive pixels and true negative pixels. The results are given below. It is not surprising to see that the sudoku images have the best results. It was actually the easiest type of images to process. For soccer and road images, the percentage of true positive pixels is lower. This is not so surprising. Indeed, during the first part of the project, one decided mainly to reject all the false lines that were detected by the method. This makes that lots of the true lines were also rejected. Therefore, if one looks at the percentage of true negative pixels, the percentages are way better in the case of soccer and road images. \n",
    "\n",
    "Finally, it will appear interesting to look at the impact of the dilation postprocessing on the performance assessment. This means, what happens if one dilates the detected edges by a factor 2, 3, 4,... In order to visualize it, one will generate the ROC curve associated to this. In a sense, it the dilation is performed with a factor 0, this means that the program will detect no true positive pixels and no false positive (0,0). If the dilation is performed with a factor really huge, the dilation will make the program will never detect any true negative or false negative pixels (1,1). The resulting plots are given below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sudoku images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                     # Numerical algorithms on arrays\n",
    "import cv2                             # OpenCV\n",
    "from matplotlib import pyplot as plt   # Plot library\n",
    "import matplotlib.cm as cm             # Image color map             \n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "import keras\n",
    "\n",
    "from Codes_report.tools import multiPlot         # A few helpers to plot multiple images\n",
    "from Codes_report.sudoku_lines import sudoku_lines_eval, roc_curve_sudoku\n",
    "from Codes_report.road_lines import road_lines_eval, roc_curve_road\n",
    "from Codes_report.soccer_lines import soccer_lines_eval, roc_curve_soccer\n",
    "from Codes_report.IOU_soccer import IOU_soccer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilation_parameter = 1\n",
    "\n",
    "true_pos, false_pos, true_neg, false_neg, lines_detected = sudoku_lines_eval(dilation_parameter)\n",
    "print(\"Pourcent of true positive pixels = \", true_pos, \"%\")\n",
    "print(\"Pourcent of false negative pixels = \", false_neg, \"%\")\n",
    "\n",
    "print(\"Pourcent of true negative pixels = \", true_neg, \"%\")\n",
    "print(\"Pourcent of false positive pixels = \", false_pos, \"%\")\n",
    "\n",
    "print(lines_detected,\"% of the Hough lines are real lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soccer images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilation_parameter = 1\n",
    "\n",
    "true_pos, false_pos, true_neg, false_neg, lines_detected = soccer_lines_eval(dilation_parameter)\n",
    "print(\"Pourcent of true positive pixels = \", true_pos, \"%\")\n",
    "print(\"Pourcent of false negative pixels = \", false_neg, \"%\")\n",
    "\n",
    "print(\"Pourcent of true negative pixels = \", true_neg, \"%\")\n",
    "print(\"Pourcent of false positive pixels = \", false_pos, \"%\")\n",
    "\n",
    "print(lines_detected,\"% of the Hough lines are real lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Road images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilation_parameter = 1\n",
    "\n",
    "true_pos, false_pos, true_neg, false_neg, lines_detected = road_lines_eval(dilation_parameter)\n",
    "print(\"Pourcent of true positive pixels = \", true_pos, \"%\")\n",
    "print(\"Pourcent of false negative pixels = \", false_neg, \"%\")\n",
    "\n",
    "print(\"Pourcent of true negative pixels = \", true_neg, \"%\")\n",
    "print(\"Pourcent of false positive pixels = \", false_pos, \"%\")\n",
    "\n",
    "print(lines_detected,\"% of the Hough lines are real lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curves "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The ROC curve in the case of the Sudoku and Soccer images show that a dilation does increase the true and false positive rates (dilation parameter going from 1 to 5). The first peak of the curve corresponds to a dilation of 1, then the two rates tend to increase. However, the false positive rate increases much faster than the true postive rate, which makes that it is better, in this case, to keep the dilation parameter equal to 1. This is sensible. In fact, increasing the width of the lines increase both the rate of the true positive pixels and the rate of the false negative pixels. \n",
    "\n",
    "However, in the case of the Road images, it appears that a dilation is really good to increase the true positive rate, as the false positive rate remains really low. So, the range of dilation_parameters is larger than for the previous cases (dilation parameter in [1,5,10,15,20]). From the curve, one can observe that a dilation parameter equal to 15 could be great to increase the performance of the line segment detection of this type of images. Beyong this value, the rate of false positive becomes too important.\n",
    "\n",
    "#### Sudoku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_vector,false_pos_vector = roc_curve_sudoku()\n",
    "\n",
    "print(true_pos_vector)\n",
    "print(false_pos_vector)\n",
    "\n",
    "plt.title(\"Sudoku ROC curve\")\n",
    "plt.plot(false_pos_vector, true_pos_vector)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soccer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_vector,false_pos_vector = roc_curve_soccer()\n",
    "\n",
    "print(true_pos_vector)\n",
    "print(false_pos_vector)\n",
    "\n",
    "plt.title(\"Soccer ROC curve\")\n",
    "plt.plot(false_pos_vector, true_pos_vector)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_vector,false_pos_vector = roc_curve_road()\n",
    "\n",
    "print(true_pos_vector)\n",
    "print(false_pos_vector)\n",
    "\n",
    "plt.title(\"Road ROC curve\")\n",
    "plt.plot(false_pos_vector, true_pos_vector)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2 : ellipse matching\n",
    "\n",
    "In this task, the main objectives are the detection of the number of ellipses on the eyes and soccer images but also to provide accurate values for their parameters, such as their location, orientation and size. \n",
    "\n",
    "\n",
    "### Data processing\n",
    "For the soccer images, based on the annotations, a bounding box enclosing all ellipse points is computed. Then the parameters of the bounding box (i.e. center coordinates, lengthn height) are fed as input to the YOLO neural network after being normalized as a function of the image dimensions)\n",
    "\n",
    "For the eye images, based on the annotations, an ellipse is fitted using the cv2.fitEllipse function. Then a binary mask image is created by drawing the (filled) resulting ellipse in white on a black image. These masks are then fed as input to the UNET neural network.\n",
    "\n",
    "### Image preprocessing\n",
    "Some preprocessing was also realized for the eye images. However, this may not be necessary since we can consider that the first layers of the neural network actually do the job of preprocessing, by blurring the image for instance. The preprocessing used consists in a median blur with a kernel size of 5 and an erosion with a kernel size of 5. We noticed that this processing allows to blur the eyelashes, which is quite useful for all the photos where the eyelashes cover part of the pupil and therefore induce some noise, which can be reduced this way. This preprocessing also allowed to decrease the noise induced by the reflection of light visible on the eye as a white spot. This white spot, when present on the pupil, induces some noise. The preprocessing allow to attenuate the white spot. \n",
    "\n",
    "Of course, this preprocessing is applied to all images from the training, evaluation and test sets, such that the images on which the neural network is tested underwent the same treatment as the images on which the network has trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'train/images/input/elps_eye_80.png'\n",
    "img = cv2.imread(image_path)\n",
    "median_kernel = 5\n",
    "ksize=5\n",
    "if ksize % 2 == 0:\n",
    "    ksize -= 1\n",
    "kernel = np.ones((ksize, ksize), np.uint8)\n",
    "img_blur = cv2.medianBlur(img, median_kernel)\n",
    "img_erd = cv2.erode(img_blur, kernel, iterations = 1)\n",
    "\n",
    "multiPlot(1, 3, (img, img_blur, img_erd),\n",
    "                     ('Original image', 'Median blur', 'Median blur + erosion'),\n",
    "                     cmap_tuple=(cm.gray, cm.gray, cm.gray))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soccer images\n",
    "\n",
    "For the soccer images, we decide to use the network Yolo to classify the ellipses on the soccer images. You Only Look Once (YOLO) is a pretrained classifier for object detection using a bounding box. We have used the configuration YOLOv3 with some modifications so that YOLO only detects the ellipses of the soccer field in a reasonable time according to the capacity of our GPU.\n",
    "\n",
    "Our dataset have been randomly split into the training set, the validation set and the test set that contains 70%, 20% and 10% of the soccer images, respectively. The training and validation sets with the corresponding positions of the ellipses in a Darknet format are used to train the classifier and compute new weights. After the training session, the classifier is used with the new obtained weights to classify the images on the test set. The analyse of the performances of the classification will be focus on the results of the test set images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eye images\n",
    "\n",
    "For the eye images, we decided to use semantic segmentation to detect the ellipse of the pupil. The architecture used is the Unet neural network. This network has first an encoding part, which captures the features of the input image by using convolutional layer and max pooling layer, followed by a decoder part, which localizes them, by using convolutional layer and upsampling.  \n",
    "The training has been performed with a GPU to lower the computational time.\n",
    "\n",
    "Our dataset have been randomly split into the training set (1838 images), the validation set (525 images) and the test set (262 images) which represents 70%, 20% and 10% of the eye dataset, respectively.  \n",
    "We decided to not use data augmentation such as random horizontal flipping in the training dataset even though we are aware that this could lead to better results.  \n",
    "\n",
    "For semantic segmentation, each training and evaluation images must have its mask ground truth.  \n",
    "We used the fitEllipse function of opencv to obtain the $a, b, x_c, y_c, \\theta$ to then draw the corresponding ellipse which will be used as mask.  \n",
    "\n",
    "The loss used during the training is the binary cross entropy $ = -\\sum (y \\log(p) + (1-y) \\log(1-p))$, where $y$ is the ground truth label and $p$ is the predicted label.  \n",
    "All the hyperparameters have been set to their default value.\n",
    "\n",
    "In order to prevent overfitting, we save the weights after an epoch if the validation loss is improved. Thus for inference, we will use the weights which gave us the best evaluation loss.  \n",
    "\n",
    "We decided to train the eye images with and without preprocessing.  \n",
    "The validation loss can be seen on the following graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = pd.read_csv(\"Data/prep.csv\", header=None)\n",
    "noprep = pd.read_csv(\"Data/noprep.csv\", header=None)\n",
    "\n",
    "p = []\n",
    "for row in prep.iterrows():\n",
    "    p.append(row[1][0])\n",
    "\n",
    "npp = []\n",
    "for row in noprep.iterrows():\n",
    "    npp.append(row[1][0])\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, 50), p, 'r', np.arange(0, 50), npp, 'g')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend(['With preprocessing', 'W/O preprocessing'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "During inference, the network predicts for each pixel the confidence it has to be true.  \n",
    "A threshold analyzis will be carried out in section 2.4 to determine what value(s) use.  \n",
    "\n",
    "Semantic segmentation gives nice and clear results for pupil detection.  \n",
    "The green pixels are the True Positive ones.  \n",
    "The red pixels are the False Positive ones.  \n",
    "The blue pixels are the False Negative ones.  \n",
    "The pixels which are not colored are the True Negative ones.  \n",
    "Thus the green and red pixels give the ground truth mask whereas the green and blue give the predicted mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'Data/test/images/input/elps_eye_272.png'\n",
    "mask_path = 'Data/test/masks/output/mask_eye_272.png'\n",
    "# Load the models\n",
    "model = load_model('unet_weights_no_preprocessing.h5')\n",
    "\n",
    "# Load the image\n",
    "image_inp = Image.open(image_path)\n",
    "\n",
    "mask_inp = Image.open(mask_path)\n",
    "mask = np.array(mask_inp)/255.\n",
    "\n",
    "# Scale the input between [0 1] and change the arrays (1, 240, 320, 3)\n",
    "image_inp = np.array(image_inp)/255.\n",
    "inp = np.expand_dims(image_inp, axis=0)\n",
    "\n",
    "# Predict the segmanted mask given the model\n",
    "y_pred = model.predict(inp, verbose=0)\n",
    "\n",
    "# Reshape the output (240, 320)\n",
    "y_pred = np.reshape(y_pred, (240, 320))\n",
    "\n",
    "threshold = 0.7\n",
    "y_pred[y_pred < threshold] = 0\n",
    "y_pred[y_pred >= threshold] = 1\n",
    "\n",
    "a, b, x, y, theta = getElpsParameters(y_pred)\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "TN = 0\n",
    "\n",
    "for i in range(240):\n",
    "    for j in range(320):\n",
    "        # True Positive in green\n",
    "        if y_pred[i][j] and mask[i][j][0]:\n",
    "            image_inp[i][j][1] = 1\n",
    "            TP += 1\n",
    "        # False Positive in red\n",
    "        if y_pred[i][j] and not mask[i][j][0]:\n",
    "            image_inp[i][j][0] = 1\n",
    "            FP += 1\n",
    "        # False Negative in blue\n",
    "        if not y_pred[i][j] and mask[i][j][0]:\n",
    "            image_inp[i][j][2] = 1\n",
    "            FN += 1\n",
    "        if not y_pred[i][j] and not mask[i][j][0]:\n",
    "            TN += 1\n",
    "\n",
    "if TP != 0 and TN != 0: \n",
    "    prec = TP/(TP + FP)\n",
    "else:\n",
    "    prec = 0\n",
    "rec = TP/(TP + FN)\n",
    "mr = FN/(FN + TP)\n",
    "acc = (TP+TN)/(TP+TN+FP+FN)  \n",
    "\n",
    "print(\"Precision : {}, Recall : {}, Miss Rate : {}, Accuracy : {}\".format(prec, rec, mr, acc))\n",
    "\n",
    "print(\"a : {}, b : {}, x : {}, y : {}, theta : {}\".format(a, b, x, y, theta))\n",
    "# image_inp[..., 0] += y_pred\n",
    "# image_inp[image_inp > 1] = 1\n",
    "\n",
    "multiPlot(1, 2, (image_inp, mask),\n",
    "                 ('Original image', 'Ground truth mask', 'Predicted'),\n",
    "                 cmap_tuple=(None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.4 :  performance assessment of the ellipse matching module\n",
    "\n",
    "### Soccer \n",
    "\n",
    "First, let's focus on the classification task. The results show that, on the test set, % of the ellipses have been well detected and classified as ellipse. Moreover, by looking at the images where the ellipse classification was not made, Only a small part of the ellipse is visible on the image. \n",
    "\n",
    "The second results show that, on the test set, % of the detected ellipses are real ellipses. In fact, Only one detected ellipse on the test set is classified wrongly but by looking at the picture, the detected ellipse is clearly an ellipse an annotation should have been made. So, the ellipse classifications of our model are all correct on the test set.\n",
    "\n",
    "Now, let's focus on the matching of the bounding boxes. To evaluate our model, the Intersection over Union (IoU) is used. This metric divide the area of the intersection of the bounding boxe of the annotations and the bounding boxe of the detected ellipse by the union of this 2 bounding boxes. The closer the value is to 1, the more similar the boxes. The result obtained for the mean values of the IoU between the bounding boxes of the test set is equal to 0.87, which is an excellent result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_detec, prop_well_class,  files_bad, mean_IOUs, list_IOU = IOU_soccer()\n",
    "\n",
    "print(\"Proportion of detected ellipses: {}\".format(prop_detec))\n",
    "print(\"Proportion of real ellipses from detected ellipses {}\".format(prop_well_class))\n",
    "print(\"mean IOU: {}\".format(mean_IOUs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pjreddie.com/darknet/yolo/\n",
    "https://arxiv.org/abs/1505.04597\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
